{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmkuhUMBm0M1"
      },
      "outputs": [],
      "source": [
        "pip install librosa numpy scikit-learn tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "def extract_features(file_path, max_pad_len=216):\n",
        "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_path, sr=None)  # Load audio file\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)  # Extract 40 MFCC features\n",
        "        pad_width = max_pad_len - mfccs.shape[1]  # Padding to ensure fixed size\n",
        "        if pad_width > 0:\n",
        "            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "        else:\n",
        "            mfccs = mfccs[:, :max_pad_len]\n",
        "        return mfccs\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "mVBDeeTcnbfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_dataset(data_dir):\n",
        "    \"\"\"Load dataset from the specified directory.\"\"\"\n",
        "    features, labels = [], []\n",
        "    for label, sub_dir in enumerate(['real', 'fake']):  # 0 for real, 1 for fake\n",
        "        sub_dir_path = os.path.join(data_dir, sub_dir)\n",
        "        for file_name in os.listdir(sub_dir_path):\n",
        "            file_path = os.path.join(sub_dir_path, file_name)\n",
        "            mfccs = extract_features(file_path)\n",
        "            if mfccs is not None:\n",
        "                features.append(mfccs)\n",
        "                labels.append(label)\n",
        "    return np.array(features), np.array(labels)"
      ],
      "metadata": {
        "id": "oFwJrxCknfJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_dir = '/content/sample_data/dataset'\n",
        "features, labels = load_dataset(data_dir)\n",
        "features = np.expand_dims(features, axis=-1)  # Add a channel dimension\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "9zB8u9Uon9ao",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(40, 216, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification (0 or 1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "uGOdxXJO8Qwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UtZfgGaGNSCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in the below code just enter the path of your sample data to check if it is fake or real\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QPvN2zULAeQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_fake(audio_file_path, model):\n",
        "    \"\"\"Classify if the audio is fake or real.\"\"\"\n",
        "    features = extract_features(audio_file_path)\n",
        "    if features is not None:\n",
        "        features = np.expand_dims(features, axis=[0, -1])  # Prepare for prediction\n",
        "        prediction = model.predict(features)\n",
        "        return \"Fake\" if prediction >= 0.5 else \"Real\"\n",
        "    return \"Error processing file\"\n",
        "\n",
        "audio_path = '/content/sample_data/dataset/fake/Alg_1_5.wav'  # enter the path of your sample data.\n",
        "print(f\"The audio is {detect_fake(audio_path, model)}\")"
      ],
      "metadata": {
        "id": "Oc4yJBF5804t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}